{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import SpeechRecognizer\n",
    "from helpers.an4_sphere_dataset import build_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json unified_vocab.json\n",
    "import json\n",
    "with open('datasets/unified_vocab.json', 'r') as f:\n",
    "    unified_vocab = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split: 759 training samples, 189 validation samples\n"
     ]
    }
   ],
   "source": [
    "# the build_dataloader function contains a collate_fn function that pads the input sequences to the maximum length in the batch\n",
    "\n",
    "train_loader, val_loader = build_dataloader(\n",
    "    data_dir=\"datasets/train\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    split=True,\n",
    "    val_ratio=0.2\n",
    ")\n",
    "\n",
    "test_loader = build_dataloader(\n",
    "    \"datasets/test\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    split=False,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpeechRecognizer(\n",
       "  (lstm): LSTM(64, 256, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout_lstm): Dropout(p=0.0, inplace=False)\n",
       "  (fc): Linear(in_features=512, out_features=107, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SpeechRecognizer(input_dim=64, hidden_dim=256, vocab_size=len(unified_vocab['word_to_idx']), dropout_rate=0.5)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ctc_loss = nn.CTCLoss(blank=0)  # index 0 is blank\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: training avg CTC loss = 7.067\n",
      "Epoch 1: validation avg CTC loss = 3.455\n",
      "Epoch 2: training avg CTC loss = 3.237\n",
      "Epoch 2: validation avg CTC loss = 3.178\n",
      "Epoch 3: training avg CTC loss = 3.072\n",
      "Epoch 3: validation avg CTC loss = 3.053\n",
      "Epoch 4: training avg CTC loss = 2.994\n",
      "Epoch 4: validation avg CTC loss = 2.985\n",
      "Epoch 5: training avg CTC loss = 2.920\n",
      "Epoch 5: validation avg CTC loss = 3.014\n",
      "Epoch 6: training avg CTC loss = 2.921\n",
      "Epoch 6: validation avg CTC loss = 2.915\n",
      "Epoch 7: training avg CTC loss = 2.977\n",
      "Epoch 7: validation avg CTC loss = 3.176\n",
      "Epoch 8: training avg CTC loss = 3.084\n",
      "Epoch 8: validation avg CTC loss = 3.058\n",
      "Epoch 9: training avg CTC loss = 2.932\n",
      "Epoch 9: validation avg CTC loss = 3.161\n",
      "Epoch 10: training avg CTC loss = 2.870\n",
      "Epoch 10: validation avg CTC loss = 2.893\n",
      "Epoch 11: training avg CTC loss = 2.688\n",
      "Epoch 11: validation avg CTC loss = 2.757\n",
      "Epoch 12: training avg CTC loss = 2.659\n",
      "Epoch 12: validation avg CTC loss = 2.761\n",
      "Epoch 13: training avg CTC loss = 2.586\n",
      "Epoch 13: validation avg CTC loss = 2.656\n",
      "Epoch 14: training avg CTC loss = 2.521\n",
      "Epoch 14: validation avg CTC loss = 2.639\n",
      "Epoch 15: training avg CTC loss = 2.440\n",
      "Epoch 15: validation avg CTC loss = 2.552\n",
      "Epoch 16: training avg CTC loss = 2.359\n",
      "Epoch 16: validation avg CTC loss = 2.654\n",
      "Epoch 17: training avg CTC loss = 2.291\n",
      "Epoch 17: validation avg CTC loss = 2.443\n",
      "Epoch 18: training avg CTC loss = 2.256\n",
      "Epoch 18: validation avg CTC loss = 2.504\n",
      "Epoch 19: training avg CTC loss = 2.319\n",
      "Epoch 19: validation avg CTC loss = 2.477\n",
      "Epoch 20: training avg CTC loss = 2.218\n",
      "Epoch 20: validation avg CTC loss = 2.539\n",
      "Epoch 21: training avg CTC loss = 2.179\n",
      "Epoch 21: validation avg CTC loss = 2.596\n",
      "Epoch 22: training avg CTC loss = 2.156\n",
      "Epoch 22: validation avg CTC loss = 2.459\n",
      "Epoch 23: training avg CTC loss = 2.110\n",
      "Epoch 23: validation avg CTC loss = 2.503\n",
      "Epoch 24: training avg CTC loss = 2.058\n",
      "Epoch 24: validation avg CTC loss = 2.562\n",
      "Epoch 25: training avg CTC loss = 2.016\n",
      "Epoch 25: validation avg CTC loss = 2.452\n",
      "Epoch 26: training avg CTC loss = 1.962\n",
      "Epoch 26: validation avg CTC loss = 2.454\n",
      "Epoch 27: training avg CTC loss = 1.917\n",
      "Epoch 27: validation avg CTC loss = 2.364\n",
      "Epoch 28: training avg CTC loss = 1.848\n",
      "Epoch 28: validation avg CTC loss = 2.461\n",
      "Epoch 29: training avg CTC loss = 1.773\n",
      "Epoch 29: validation avg CTC loss = 2.422\n",
      "Epoch 30: training avg CTC loss = 1.764\n",
      "Epoch 30: validation avg CTC loss = 2.452\n",
      "Epoch 31: training avg CTC loss = 1.706\n",
      "Epoch 31: validation avg CTC loss = 2.538\n",
      "Epoch 32: training avg CTC loss = 1.693\n",
      "Epoch 32: validation avg CTC loss = 2.450\n",
      "Epoch 33: training avg CTC loss = 1.738\n",
      "Epoch 33: validation avg CTC loss = 2.524\n",
      "Epoch 34: training avg CTC loss = 1.641\n",
      "Epoch 34: validation avg CTC loss = 2.524\n",
      "Epoch 35: training avg CTC loss = 1.540\n",
      "Epoch 35: validation avg CTC loss = 2.475\n",
      "Epoch 36: training avg CTC loss = 1.495\n",
      "Epoch 36: validation avg CTC loss = 2.579\n",
      "Epoch 37: training avg CTC loss = 1.413\n",
      "Epoch 37: validation avg CTC loss = 2.499\n",
      "Epoch 38: training avg CTC loss = 1.345\n",
      "Epoch 38: validation avg CTC loss = 2.695\n",
      "Epoch 39: training avg CTC loss = 1.320\n",
      "Epoch 39: validation avg CTC loss = 2.675\n",
      "Epoch 40: training avg CTC loss = 1.245\n",
      "Epoch 40: validation avg CTC loss = 2.633\n",
      "Epoch 41: training avg CTC loss = 1.185\n",
      "Epoch 41: validation avg CTC loss = 2.691\n",
      "Epoch 42: training avg CTC loss = 1.159\n",
      "Epoch 42: validation avg CTC loss = 2.794\n",
      "Epoch 43: training avg CTC loss = 1.086\n",
      "Epoch 43: validation avg CTC loss = 2.842\n",
      "Epoch 44: training avg CTC loss = 1.098\n",
      "Epoch 44: validation avg CTC loss = 2.806\n",
      "Epoch 45: training avg CTC loss = 1.028\n",
      "Epoch 45: validation avg CTC loss = 2.790\n",
      "Epoch 46: training avg CTC loss = 0.953\n",
      "Epoch 46: validation avg CTC loss = 2.804\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m model.train()\n\u001b[32m      4\u001b[39m total_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmel_specs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmel_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_lengths\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmel_specs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_ids\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmel_specs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_ids\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_ids\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ultralytics/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    629\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    630\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m631\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    634\u001b[39m         \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    635\u001b[39m         \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ultralytics/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    673\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    674\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m675\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    676\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    677\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ultralytics/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     51\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ultralytics/lib/python3.11/site-packages/torch/utils/data/dataset.py:399\u001b[39m, in \u001b[36mSubset.__getitems__\u001b[39m\u001b[34m(self, indices)\u001b[39m\n\u001b[32m    397\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ultralytics/lib/python3.11/site-packages/torch/utils/data/dataset.py:399\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    397\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m.dataset[\u001b[38;5;28mself\u001b[39m.indices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 6. Training loop with CTC loss\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for mel_specs, vocab_ids, mel_lengths, vocab_lengths in train_loader:\n",
    "        mel_specs, vocab_ids = mel_specs.to(device), vocab_ids.to(device)\n",
    "\n",
    "        B, T = vocab_ids.size()\n",
    "\n",
    "        # Forward pass\n",
    "        log_probs = model(mel_specs)\n",
    "\n",
    "        log_probs_ctc = log_probs.transpose(0, 1)  # https://pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html\n",
    "        # The CTC loss function needs actual lengths, not masks\n",
    "        loss = ctc_loss(\n",
    "            log_probs_ctc,\n",
    "            vocab_ids,\n",
    "            input_lengths=mel_lengths,  # Use the actual mel spectrogram lengths\n",
    "            target_lengths=vocab_lengths  # Use the actual target sequence lengths\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * B\n",
    "    print(f\"Epoch {epoch+1}: training avg CTC loss = {total_loss/len(train_loader.dataset):.3f}\")\n",
    "    # Validate on validation set\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for mel_specs, vocab_ids, mel_lengths, vocab_lengths in val_loader:\n",
    "            mel_specs, vocab_ids = mel_specs.to(device), vocab_ids.to(device)\n",
    "\n",
    "            B, T = vocab_ids.size()\n",
    "\n",
    "            log_probs = model(mel_specs)\n",
    "            log_probs_ctc = log_probs.transpose(0, 1)  # https://pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html\n",
    "            # The CTC loss function needs actual lengths, not masks\n",
    "            loss = ctc_loss(\n",
    "                log_probs_ctc,\n",
    "                vocab_ids,\n",
    "                input_lengths=mel_lengths,  # Use the actual mel spectrogram lengths\n",
    "                target_lengths=vocab_lengths  # Use the actual target sequence lengths\n",
    "            )\n",
    "            total_loss += loss.item() * B\n",
    "        print(f\"Epoch {epoch+1}: validation avg CTC loss = {total_loss/len(val_loader.dataset):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_greedy_decode(log_probs, idx_to_char):\n",
    "    \"\"\"\n",
    "    Decode CTC output using greedy algorithm\n",
    "    \n",
    "    Args:\n",
    "        log_probs: tensor of shape (batch_size, seq_len, num_classes)\n",
    "        idx_to_char: dictionary mapping indices to characters\n",
    "    \n",
    "    Returns:\n",
    "        list of decoded strings, one per batch item\n",
    "    \"\"\"\n",
    "    # Get the most likely class at each timestep\n",
    "    pred_indices = log_probs.argmax(dim=2).cpu().numpy()  # (batch_size, seq_len)\n",
    "    \n",
    "    batch_results = []\n",
    "    for indices in pred_indices:  # Process each sequence in the batch\n",
    "        # Collapse repeats and remove blanks\n",
    "        prev = None\n",
    "        pred_words = []\n",
    "        for idx in indices:\n",
    "            if idx != prev and idx != 0:  # 0 is CTC blank\n",
    "                pred_words.append(idx)\n",
    "            prev = idx\n",
    "        batch_results.append(pred_words)\n",
    "    \n",
    "    return batch_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test WER: 100.00%\n"
     ]
    }
   ],
   "source": [
    "total_words, total_errors = 0, 0\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for mel_specs, vocab_ids, mel_lengths, vocab_lengths in test_loader:\n",
    "        mel_specs, vocab_ids = mel_specs.to(device), vocab_ids.to(device)\n",
    "        log_probs = model(mel_specs)  # (T,1,vocab)\n",
    "        hyp_words = ctc_greedy_decode(log_probs, unified_vocab['idx_to_word'])\n",
    "        # Compute word error rate for this utterance\n",
    "        ref_words = vocab_ids\n",
    "        # Levenshtein edit distance for words:\n",
    "        # Initialize DP table\n",
    "        d = [[0]*(len(hyp_words)+1) for _ in range(len(ref_words)+1)]\n",
    "        for i in range(len(ref_words)+1): \n",
    "            d[i][0] = i\n",
    "        for j in range(len(hyp_words)+1): \n",
    "            d[0][j] = j\n",
    "        for i, rw in enumerate(ref_words, start=1):\n",
    "            for j, hw in enumerate(hyp_words, start=1):\n",
    "                cost = 0 if rw == hw else 1\n",
    "                d[i][j] = min(d[i-1][j] + 1, d[i][j-1] + 1, d[i-1][j-1] + cost)\n",
    "        total_errors += d[len(ref_words)][len(hyp_words)]\n",
    "        total_words += len(ref_words)\n",
    "wer = total_errors / total_words\n",
    "print(f\"Test WER: {wer*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ultralytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
