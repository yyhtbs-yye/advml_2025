{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6MPjfT5NrKQ"
      },
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "  <a href=\"https://ultralytics.com/yolo\" target=\"_blank\">\n",
        "    <img width=\"1024\", src=\"https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png\"></a>\n",
        "\n",
        "  [‰∏≠Êñá](https://docs.ultralytics.com/zh/) | [ÌïúÍµ≠Ïñ¥](https://docs.ultralytics.com/ko/) | [Êó•Êú¨Ë™û](https://docs.ultralytics.com/ja/) | [–†—É—Å—Å–∫–∏–π](https://docs.ultralytics.com/ru/) | [Deutsch](https://docs.ultralytics.com/de/) | [Fran√ßais](https://docs.ultralytics.com/fr/) | [Espa√±ol](https://docs.ultralytics.com/es/) | [Portugu√™s](https://docs.ultralytics.com/pt/) | [T√ºrk√ße](https://docs.ultralytics.com/tr/) | [Ti·∫øng Vi·ªát](https://docs.ultralytics.com/vi/) | [ÿßŸÑÿπÿ±ÿ®Ÿäÿ©](https://docs.ultralytics.com/ar/)\n",
        "\n",
        "  <a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n",
        "  <a href=\"https://colab.research.google.com/github/ultralytics/notebooks/blob/main/notebooks/inference-with-meta-sam-and-sam2-using-ultralytics-python-package.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n",
        "\n",
        "\n",
        "  <a href=\"https://ultralytics.com/discord\"><img alt=\"Discord\" src=\"https://img.shields.io/discord/1089800235347353640?logo=discord&logoColor=white&label=Discord&color=blue\"></a>\n",
        "  <a href=\"https://community.ultralytics.com\"><img alt=\"Ultralytics Forums\" src=\"https://img.shields.io/discourse/users?server=https%3A%2F%2Fcommunity.ultralytics.com&logo=discourse&label=Forums&color=blue\"></a>\n",
        "  <a href=\"https://reddit.com/r/ultralytics\"><img alt=\"Ultralytics Reddit\" src=\"https://img.shields.io/reddit/subreddit-subscribers/ultralytics?style=flat&logo=reddit&logoColor=white&label=Reddit&color=blue\"></a>\n",
        "  \n",
        "  Welcome to the Inference with Meta [Segment Anything (SAM)](https://docs.ultralytics.com/models/sam/) using [Ultralytics](https://github.com/ultralytics/ultralytics) [Python Package](https://pypi.org/project/ultralytics/) üöÄ notebook! The Segment Anything Model, or SAM, is a cutting-edge image segmentation model that allows for promptable segmentation, providing unparalleled versatility in image analysis tasks.  We hope that the resources in this notebook will help you get the most out of SAM and SAM2. Please browse the <a href=\"https://docs.ultralytics.com/\">Docs</a> for details, raise an issue on <a href=\"https://github.com/ultralytics/ultralytics\">GitHub</a> for support, and join our <a href=\"https://ultralytics.com/discord\">Discord</a> community for questions and discussions!</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Segment Anything Model (SAM)\n",
        "\n",
        "This notebook serves as a starting point for using the Segment Anything (SAM) or [Segment Anything 2 (SAM2)](https://docs.ultralytics.com/models/sam-2/) model with the [Ultralytics Python Package](https://pypi.org/project/ultralytics/)."
      ],
      "metadata": {
        "id": "7EM2nwU4jshF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to SAM and Segment Anything 2\n",
        "\n",
        "SAM is a groundbreaking project that introduces a novel model, task, and dataset for image segmentation. Its advanced design allows it to adapt to new image distributions and tasks without prior knowledge, a feature known as zero-shot transfer.\n",
        "\n",
        "- Trained on the expansive SA-1B dataset, which contains more than 1 billion masks spread over 11 million carefully curated images, SAM has displayed impressive zero-shot performance, surpassing previous fully supervised results in many cases.\n",
        "\n",
        "**[SAM 2](https://docs.ultralytics.com/models/sam-2/#key-features)**, the successor to Meta's Segment Anything Model (SAM), is a cutting-edge tool designed for comprehensive object segmentation in both images and videos. It excels in handling complex visual data through a unified, promptable model architecture that supports real-time processing and zero-shot generalization."
      ],
      "metadata": {
        "id": "xypoYW_oYZAf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![SAM2 Inference Results](https://github.com/user-attachments/assets/193d69bc-ae6a-43c4-98d6-9d7a4e86aa99)"
      ],
      "metadata": {
        "id": "R4SICbq5Yalg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mGmQbAO5pQb"
      },
      "source": [
        "## Setup\n",
        "\n",
        "pip install `ultralytics` and [dependencies](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) and check software and hardware.\n",
        "\n",
        "[![PyPI - Version](https://img.shields.io/pypi/v/ultralytics?logo=pypi&logoColor=white)](https://pypi.org/project/ultralytics/) [![Downloads](https://static.pepy.tech/badge/ultralytics)](https://www.pepy.tech/projects/ultralytics) [![PyPI - Python Version](https://img.shields.io/pypi/pyversions/ultralytics?logo=python&logoColor=gold)](https://pypi.org/project/ultralytics/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbvMlHd_QwMG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8f1494f-35f3-4316-d3f4-42c9466e3a49"
      },
      "source": [
        "!pip install ultralytics\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.128 üöÄ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Setup complete ‚úÖ (2 CPUs, 12.7 GB RAM, 41.1/112.6 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Segment Everything\n",
        "\n",
        "Using SAM and SAM2 models, you can [segment](https://docs.ultralytics.com/tasks/segment/) the entire image or video content without specific prompts."
      ],
      "metadata": {
        "id": "xE6ntKojSfSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import SAM\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "# Load the SAM2.1 model\n",
        "model = SAM(\"sam2.1_b.pt\")\n",
        "\n",
        "# Display model information (optional)\n",
        "model.info()\n",
        "\n",
        "# Define the video or frame sequence input\n",
        "video_path = \"/content/rte_far_seg_1_output.mp4\"  # Replace with your video file path\n",
        "# Alternatively, for a folder of frames:\n",
        "# frame_folder = \"path/to/frames/\"  # Directory with frames like frame_00000000.png\n",
        "\n",
        "# Initialize video capture\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "if not cap.isOpened():\n",
        "    print(\"Error: Could not open video.\")\n",
        "    exit()\n",
        "\n",
        "# Define output directory for saving images\n",
        "output_dir = \"output_tracked_frames\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Define initial prompt for the object to track (e.g., a point)\n",
        "initial_prompt = {\"points\": [[933, 419]], \"labels\": [1]}  # Positive point for object\n",
        "\n",
        "# Process video frames for tracking\n",
        "frame_idx = 0\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Convert frame to format suitable for SAM (BGR to RGB)\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Run inference with tracking\n",
        "    # For the first frame, provide the initial prompt\n",
        "    # For subsequent frames, SAM2/SAM2.1 uses temporal context automatically\n",
        "    results = model(frame_rgb, **initial_prompt if frame_idx == 0 else {})\n",
        "\n",
        "    # Visualize results\n",
        "    annotated_frame = results[0].plot()  # Plot segmentation masks on the frame\n",
        "    annotated_frame = cv2.cvtColor(annotated_frame, cv2.COLOR_RGB2BGR)  # Convert back to BGR for OpenCV\n",
        "\n",
        "    # Save the annotated frame as an image\n",
        "    output_path = os.path.join(output_dir, f\"frame_{frame_idx:08d}.png\")\n",
        "    cv2.imwrite(output_path, annotated_frame)\n",
        "\n",
        "    # # Optional: Display the frame (comment out if not needed)\n",
        "    # cv2.imshow(\"Tracked Frame\", annotated_frame)\n",
        "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
        "        break\n",
        "\n",
        "    frame_idx += 1\n",
        "\n",
        "# Release resources\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "print(f\"Tracking completed. Images saved to {output_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "UtLkkQmX14Wz",
        "outputId": "ba0d459a-8f51-4852-8783-13346b68ac8d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model summary: 403 layers, 80,850,178 parameters, 80,850,178 gradients\n",
            "\n",
            "0: 1024x1024 1 0, 310.5ms\n",
            "Speed: 8.6ms preprocess, 310.5ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 1024)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "DisabledFunctionError",
          "evalue": "cv2.imshow() is disabled in Colab, because it causes Jupyter sessions\nto crash; see https://github.com/jupyter/notebook/issues/3935.\nAs a substitution, consider using\n  from google.colab.patches import cv2_imshow\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mDisabledFunctionError\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-f34c963bbd72>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;31m# Optional: Display the frame (comment out if not needed)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tracked Frame\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotated_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;36m0xFF\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"q\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_import_hooks/_cv2.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mDisabledFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDisabledFunctionError\u001b[0m: cv2.imshow() is disabled in Colab, because it causes Jupyter sessions\nto crash; see https://github.com/jupyter/notebook/issues/3935.\nAs a substitution, consider using\n  from google.colab.patches import cv2_imshow\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_snippet",
                "actionText": "Search Snippets for cv2.imshow",
                "snippetFilter": "cv2.imshow"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Segment everything results](https://github.com/user-attachments/assets/5d00c0e6-42c3-4f23-9975-1340d9b866f5)"
      ],
      "metadata": {
        "id": "-RfG6TlvwEgd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Segment Anything\n",
        "\n",
        "You can [segment](https://docs.ultralytics.com/tasks/segment/) specific objects in an image or video using different prompts, such as bounding box and point prompts."
      ],
      "metadata": {
        "id": "fBZnd9l43_zc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bounding box prompt\n",
        "\n",
        "The `bbox_prompt` refers to a bounding box input that guides the model in segmenting a specific object within an image. In the example below, you will segment only the bus by providing the bounding box coordinates."
      ],
      "metadata": {
        "id": "y1XKu_Y48s8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import SAM\n",
        "\n",
        "# Load a model\n",
        "model = SAM(\"sam2.1_b.pt\")\n",
        "\n",
        "# Run inference with bboxes prompt (Provide the bounding box coordinates\n",
        "# for the bus area, ensuring that only bus is segmented in the entire image)\n",
        "results = model(\"https://ultralytics.com/images/bus.jpg\",\n",
        "                bboxes=[3.8328723907470703, 229.35601806640625,\n",
        "                        796.2098999023438, 728.4313354492188])\n",
        "\n",
        "results[0].show()  # Display results"
      ],
      "metadata": {
        "id": "AM7CLQlA8vXT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "956e0d88-5ec9-4e57-cf95-93a4008d66b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Found https://ultralytics.com/images/bus.jpg locally at bus.jpg\n",
            "image 1/1 /content/bus.jpg: 1024x1024 1 0, 334.1ms\n",
            "Speed: 7.8ms preprocess, 334.1ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Segment anything with bounding box prompt results](https://github.com/user-attachments/assets/4833d1fe-7990-4829-83d9-dc6e43b9d08c)"
      ],
      "metadata": {
        "id": "GUSR-0eowfaU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Point prompt\n",
        "\n",
        "The `point_prompt` refers to a specific point input (x, y) that guides the Segment Anything Model (SAM) in segmenting an object within an image. Instead of providing a bounding box, you can indicate an object by selecting a point on it, and SAM will generate a segmentation mask around that point."
      ],
      "metadata": {
        "id": "5YnEUSxf8v2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import SAM\n",
        "\n",
        "# Load a model\n",
        "model = SAM(\"sam2.1_b.pt\")\n",
        "\n",
        "# Run inference with point prompt (Provide the point coordinates for the\n",
        "# person area, ensuring that only the person is segmented in the entire image)\n",
        "results = model(\"https://ultralytics.com/images/bus.jpg\",\n",
        "                points=[34, 714])\n",
        "\n",
        "results[0].show()  # Display results"
      ],
      "metadata": {
        "id": "KysZKUja5vvM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f9edc75-6fe5-40ae-dfb5-d2a611652f92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Found https://ultralytics.com/images/bus.jpg locally at bus.jpg\n",
            "image 1/1 /content/bus.jpg: 1024x1024 1 0, 312.3ms\n",
            "Speed: 6.4ms preprocess, 312.3ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Segment anything with point prompt results](https://github.com/user-attachments/assets/fc218266-1e1a-4e31-a0e3-de4de55cb13c)"
      ],
      "metadata": {
        "id": "SHZWtqBWwkXS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multiple points prompt\n",
        "\n",
        "Multiple `point_prompt` inputs refer to specific points (x, y) that serve as prompts to guide the Segment Anything Model (SAM) in segmenting multiple objects within an image."
      ],
      "metadata": {
        "id": "wZBthWVlncoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import SAM\n",
        "\n",
        "# Load a model\n",
        "model = SAM(\"sam2.1_b.pt\")\n",
        "\n",
        "# Run inference with multiple point prompts (Provide the points coordinates for\n",
        "# person area, ensuring that only the person is segmented in the entire image)\n",
        "results = model(\"https://ultralytics.com/images/bus.jpg\",\n",
        "                points=[[34, 714], [283, 634]])\n",
        "\n",
        "results[0].show()  # Display results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJF6ZgjpnqYK",
        "outputId": "6b64d5b9-e010-45f9-e88a-91842246c092"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Found https://ultralytics.com/images/bus.jpg locally at bus.jpg\n",
            "image 1/1 /content/bus.jpg: 1024x1024 1 0, 1 1, 315.8ms\n",
            "Speed: 7.4ms preprocess, 315.8ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Segment anything with multiple points prompt results](https://github.com/user-attachments/assets/fd7f7d78-f79b-401f-ad03-e129e8dc89dd)"
      ],
      "metadata": {
        "id": "JCYM1yruwnL7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Auto Annotation using Segment Anything Model\n",
        "\n",
        "[Auto-annotation](https://docs.ultralytics.com/reference/data/annotator/#ultralytics.data.annotator.auto_annotate) is a core feature of SAM, enabling users to create a segmentation dataset with a pre-trained detection model. It streamlines the annotation process by quickly and accurately labeling large image sets, eliminating the need for labor-intensive manual annotation.\n",
        "\n",
        "- The example below uses two images from Ultralytics assets for auto-annotation, but you can use your own. Just create a folder, add the images you want to auto-annotate, and pass the folder path to the `auto_annotate` function via the `data` argument."
      ],
      "metadata": {
        "id": "3NoG8H9bpuiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics.data.annotator import auto_annotate\n",
        "\n",
        "# Use the Ultralytics sample dataset (You can use your own images)\n",
        "from ultralytics.utils.downloads import safe_download\n",
        "images = [\"bus.jpg\", \"zidane.jpg\"]\n",
        "for img in images:\n",
        "  path = safe_download(f\"https://ultralytics.com/assets/{img}\", dir=\"assets\")\n",
        "\n",
        "# return the annotation in the Ultralytics YOLO segmentation format.\n",
        "# output directory i.e assets_auto_annotate_labels\n",
        "auto_annotate(data=\"/content/assets\",\n",
        "              det_model=\"yolo11x.pt\",\n",
        "              sam_model=\"sam_b.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-rVDIE_pnnx",
        "outputId": "9e98f5a6-ffa9-448a-92c0-ade2c50c5144"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://ultralytics.com/assets/bus.jpg to 'assets/bus.jpg'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 134k/134k [00:00<00:00, 6.56MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://ultralytics.com/assets/zidane.jpg to 'assets/zidane.jpg'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49.2k/49.2k [00:00<00:00, 8.62MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x.pt to 'yolo11x.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 109M/109M [00:00<00:00, 342MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/sam_b.pt to 'sam_b.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 358M/358M [00:01<00:00, 200MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "image 1/2 /content/assets/bus.jpg: 640x480 4 persons, 1 bus, 71.2ms\n",
            "image 2/2 /content/assets/zidane.jpg: 384x640 2 persons, 3 ties, 33.6ms\n",
            "Speed: 2.8ms preprocess, 52.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Additional Resources  \n",
        "\n",
        "üîπ Segment Anything 2 Documentation: [üìñ Read here](https://docs.ultralytics.com/models/sam-2/)  \n",
        "üîπ SAM2 Blog: [üìù Explore applications](https://www.ultralytics.com/blog/applications-of-meta-ai-segment-anything-model-2-sam-2)"
      ],
      "metadata": {
        "id": "yOqQpo4zt1Rb"
      }
    }
  ]
}