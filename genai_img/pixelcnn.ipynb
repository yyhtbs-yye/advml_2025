{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admyyh/miniconda3/envs/ultralytics/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "# Import einops if not already imported\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedConv2d(nn.Conv2d):\n",
    "    def __init__(self, mask_type, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True):\n",
    "        super(MaskedConv2d, self).__init__(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)\n",
    "        self.register_buffer('mask', self.weight.data.clone())\n",
    "        _, _, kH, kW = self.weight.size()\n",
    "        self.mask.fill_(1)\n",
    "        self.mask[:, :, kH // 2, kW // 2 + (mask_type == 'B'):] = 0\n",
    "        self.mask[:, :, kH // 2 + 1:] = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.weight.data *= self.mask\n",
    "        return super(MaskedConv2d, self).forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiagonalLSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(DiagonalLSTMCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Input to hidden weights\n",
    "        self.W_i = nn.Conv2d(input_size, hidden_size, kernel_size=1)\n",
    "        self.W_f = nn.Conv2d(input_size, hidden_size, kernel_size=1)\n",
    "        self.W_o = nn.Conv2d(input_size, hidden_size, kernel_size=1)\n",
    "        self.W_g = nn.Conv2d(input_size, hidden_size, kernel_size=1)\n",
    "        \n",
    "        # Hidden to hidden weights (masked)\n",
    "        self.U_i = MaskedConv2d('B', hidden_size, hidden_size, kernel_size=3, padding=1)\n",
    "        self.U_f = MaskedConv2d('B', hidden_size, hidden_size, kernel_size=3, padding=1)\n",
    "        self.U_o = MaskedConv2d('B', hidden_size, hidden_size, kernel_size=3, padding=1)\n",
    "        self.U_g = MaskedConv2d('B', hidden_size, hidden_size, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Biases\n",
    "        self.b_i = nn.Parameter(torch.zeros(hidden_size))\n",
    "        self.b_f = nn.Parameter(torch.zeros(hidden_size))\n",
    "        self.b_o = nn.Parameter(torch.zeros(hidden_size))\n",
    "        self.b_g = nn.Parameter(torch.zeros(hidden_size))\n",
    "        \n",
    "    def forward(self, x, state):\n",
    "        h, c = state\n",
    "        \n",
    "        # Calculate gates\n",
    "        i = torch.sigmoid(self.W_i(x) + self.U_i(h) + self.b_i.view(1, -1, 1, 1))\n",
    "        f = torch.sigmoid(self.W_f(x) + self.U_f(h) + self.b_f.view(1, -1, 1, 1))\n",
    "        o = torch.sigmoid(self.W_o(x) + self.U_o(h) + self.b_o.view(1, -1, 1, 1))\n",
    "        g = torch.tanh(self.W_g(x) + self.U_g(h) + self.b_g.view(1, -1, 1, 1))\n",
    "        \n",
    "        # Update cell and hidden state\n",
    "        c_new = f * c + i * g\n",
    "        h_new = o * torch.tanh(c_new)\n",
    "        \n",
    "        return h_new, c_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiagonalLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
    "        super(DiagonalLSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.cells = nn.ModuleList([DiagonalLSTMCell(\n",
    "            input_size if i == 0 else hidden_size, \n",
    "            hidden_size) for i in range(num_layers)])\n",
    "        \n",
    "    def forward(self, x, states=None):\n",
    "        batch_size, _, height, width = x.size()\n",
    "        \n",
    "        if states is None:\n",
    "            states = [(torch.zeros(batch_size, self.hidden_size, height, width).to(x.device),\n",
    "                       torch.zeros(batch_size, self.hidden_size, height, width).to(x.device)) \n",
    "                      for _ in range(self.num_layers)]\n",
    "        \n",
    "        new_states = []\n",
    "        for i, cell in enumerate(self.cells):\n",
    "            h, c = states[i]\n",
    "            new_h, new_c = cell(x if i == 0 else new_h, (h, c))\n",
    "            new_states.append((new_h, new_c))\n",
    "        \n",
    "        return new_h, new_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelCNN(nn.Module):\n",
    "    def __init__(self, input_channels=3, hidden_size=128, num_layers=2, num_classes=256):\n",
    "        super(PixelCNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Initial convolution to process input\n",
    "        self.input_conv = MaskedConv2d('A', input_channels, hidden_size, kernel_size=7, padding=3)\n",
    "        \n",
    "        # DiagonalLSTM layers\n",
    "        self.lstm = DiagonalLSTM(hidden_size, hidden_size, num_layers)\n",
    "        \n",
    "        # Output convolutions\n",
    "        self.output_conv1 = nn.Conv2d(hidden_size, hidden_size, kernel_size=1)\n",
    "        self.output_conv2 = nn.Conv2d(hidden_size, input_channels * num_classes, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, channels, height, width = x.size()\n",
    "        \n",
    "        # Initial convolution\n",
    "        x = self.input_conv(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # DiagonalLSTM\n",
    "        x, _ = self.lstm(x)\n",
    "        \n",
    "        # Output layers\n",
    "        x = F.relu(self.output_conv1(x))\n",
    "        x = self.output_conv2(x)\n",
    "        \n",
    "        # Reshape output for softmax over color channels\n",
    "        x = x.view(batch_size, channels, self.num_classes, height, width)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def sample(self, batch_size=1, image_size=(28, 28), channels=1, device='cpu'):\n",
    "        sample = torch.zeros(batch_size, channels, *image_size).to(device)\n",
    "        \n",
    "        # Generate image pixel by pixel\n",
    "        for i in range(image_size[0]):\n",
    "            for j in range(image_size[1]):\n",
    "                for c in range(channels):\n",
    "                    output = self.forward(sample)\n",
    "                    probs = F.softmax(output[:, c, :, i, j], dim=1)\n",
    "                    sample[:, c, i, j] = torch.squeeze(torch.multinomial(probs, 1).float() / (self.num_classes - 1))\n",
    "        \n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Data loading\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PixelCNN(\n",
       "  (input_conv): MaskedConv2d(1, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "  (lstm): DiagonalLSTM(\n",
       "    (cells): ModuleList(\n",
       "      (0-1): 2 x DiagonalLSTMCell(\n",
       "        (W_i): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (W_f): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (W_o): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (W_g): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (U_i): MaskedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (U_f): MaskedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (U_o): MaskedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (U_g): MaskedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output_conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (output_conv2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = PixelCNN(input_channels=1, hidden_size=128, num_layers=2, num_classes=256)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, _) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        batch_size, channels, height, width = inputs.size()\n",
    "        \n",
    "        # Quantize inputs to match output classes\n",
    "        targets = (inputs * (model.num_classes - 1)).long()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Rearrange outputs to (batch * height * width * channels, num_classes)\n",
    "        outputs_flat = rearrange(outputs, 'b c n h w -> (b h w c) n')\n",
    "        \n",
    "        # Rearrange targets to (batch * height * width * channels)\n",
    "        targets_flat = rearrange(targets, 'b c h w -> (b h w c)')\n",
    "    \n",
    "        loss = criterion(outputs_flat, targets_flat)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % 100 == 99:\n",
    "            print(f'Epoch {epoch+1}, Batch {i+1}, Loss: {running_loss/100:.4f}')\n",
    "            running_loss = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    samples = model.sample(batch_size=16, image_size=(28, 28), device=device)\n",
    "    # draw the sample using matplotlib\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    for i in range(16):\n",
    "        plt.subplot(4, 4, i + 1)\n",
    "        plt.imshow(samples[i].view(samples.shape[-2], samples.shape[-1]).cpu().numpy(), cmap='gray')\n",
    "        plt.axis('off')\n",
    "# Save model\n",
    "torch.save(model.state_dict(), 'pixelcnn_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ultralytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
