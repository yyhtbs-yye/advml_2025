{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from helpers.ud_english_ewt_dataset import build_dataloader\n",
    "from model import POSTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df = pd.read_csv(\"datasets/unified_vocab.tsv\", sep='\\t')\n",
    "word_vocab = {row['token']: row['index'] for _, row in word_df.iterrows()}\n",
    "\n",
    "# Load tag vocabulary from tsv\n",
    "tag_df = pd.read_csv(\"datasets/tag_vocab.tsv\", sep='\\t')\n",
    "tag_vocab = {row['tag']: row['index'] for _, row in tag_df.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split: 8781 training samples, 3762 validation samples\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader = build_dataloader(\n",
    "    \"datasets/train_improved.tsv\", \n",
    "    batch_size=BATCH_SIZE,\n",
    "    split=True,\n",
    "    shuffle=True,\n",
    "    val_ratio=0.3\n",
    ")\n",
    "\n",
    "test_loader = build_dataloader(\n",
    "    \"datasets/test_improved.tsv\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    split=False,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admyyh/miniconda3/envs/ultralytics/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "POSTagger(\n",
       "  (embed): Embedding(19553, 64)\n",
       "  (dropout_emb): Dropout(p=0.5, inplace=False)\n",
       "  (lstm): LSTM(64, 32, batch_first=True, dropout=0.5, bidirectional=True)\n",
       "  (dropout_lstm): Dropout(p=0.5, inplace=False)\n",
       "  (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc): Linear(in_features=64, out_features=17, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = POSTagger(vocab_size=len(word_vocab), tag_count=len(tag_vocab), emb_dim=64, hidden_dim=32)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()  # tag classification loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(lengths, max_len):\n",
    "    mask = torch.zeros(len(lengths), max_len, device=lengths.device, dtype=torch.bool)\n",
    "    for i, length in enumerate(lengths):\n",
    "        # Clamp length to valid range\n",
    "        valid_length = min(max(length.item(), 0), max_len)\n",
    "        mask[i, :valid_length] = 1\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: training loss = 1.805\n",
      "Epoch 1: validation loss = 0.586\n",
      "Epoch 2: training loss = 1.225\n",
      "Epoch 2: validation loss = 0.464\n",
      "Epoch 3: training loss = 1.011\n",
      "Epoch 3: validation loss = 0.402\n",
      "Epoch 4: training loss = 0.878\n",
      "Epoch 4: validation loss = 0.355\n",
      "Epoch 5: training loss = 0.778\n",
      "Epoch 5: validation loss = 0.317\n",
      "Epoch 6: training loss = 0.690\n",
      "Epoch 6: validation loss = 0.283\n",
      "Epoch 7: training loss = 0.615\n",
      "Epoch 7: validation loss = 0.253\n",
      "Epoch 8: training loss = 0.552\n",
      "Epoch 8: validation loss = 0.229\n",
      "Epoch 9: training loss = 0.490\n",
      "Epoch 9: validation loss = 0.210\n",
      "Epoch 10: training loss = 0.438\n",
      "Epoch 10: validation loss = 0.197\n",
      "Epoch 11: training loss = 0.397\n",
      "Epoch 11: validation loss = 0.187\n",
      "Epoch 12: training loss = 0.363\n",
      "Epoch 12: validation loss = 0.179\n",
      "Epoch 13: training loss = 0.336\n",
      "Epoch 13: validation loss = 0.176\n",
      "Epoch 14: training loss = 0.314\n",
      "Epoch 14: validation loss = 0.171\n",
      "Epoch 15: training loss = 0.298\n",
      "Epoch 15: validation loss = 0.168\n",
      "Epoch 16: training loss = 0.284\n",
      "Epoch 16: validation loss = 0.168\n",
      "Epoch 17: training loss = 0.271\n",
      "Epoch 17: validation loss = 0.166\n",
      "Epoch 18: training loss = 0.259\n",
      "Epoch 18: validation loss = 0.163\n",
      "Epoch 19: training loss = 0.254\n",
      "Epoch 19: validation loss = 0.161\n",
      "Epoch 20: training loss = 0.242\n",
      "Epoch 20: validation loss = 0.160\n",
      "Epoch 21: training loss = 0.234\n",
      "Epoch 21: validation loss = 0.160\n",
      "Epoch 22: training loss = 0.230\n",
      "Epoch 22: validation loss = 0.158\n",
      "Epoch 23: training loss = 0.223\n",
      "Epoch 23: validation loss = 0.156\n",
      "Epoch 24: training loss = 0.218\n",
      "Epoch 24: validation loss = 0.156\n",
      "Epoch 25: training loss = 0.212\n",
      "Epoch 25: validation loss = 0.156\n",
      "Epoch 26: training loss = 0.207\n",
      "Epoch 26: validation loss = 0.158\n",
      "Epoch 27: training loss = 0.201\n",
      "Epoch 27: validation loss = 0.154\n",
      "Epoch 28: training loss = 0.200\n",
      "Epoch 28: validation loss = 0.154\n",
      "Epoch 29: training loss = 0.194\n",
      "Epoch 29: validation loss = 0.156\n",
      "Epoch 30: training loss = 0.188\n",
      "Epoch 30: validation loss = 0.155\n",
      "Epoch 31: training loss = 0.185\n",
      "Epoch 31: validation loss = 0.154\n",
      "Epoch 32: training loss = 0.186\n",
      "Epoch 32: validation loss = 0.154\n",
      "Epoch 33: training loss = 0.178\n",
      "Epoch 33: validation loss = 0.154\n",
      "Epoch 34: training loss = 0.176\n",
      "Epoch 34: validation loss = 0.157\n",
      "Epoch 35: training loss = 0.174\n",
      "Epoch 35: validation loss = 0.155\n",
      "Epoch 36: training loss = 0.172\n",
      "Epoch 36: validation loss = 0.154\n",
      "Epoch 37: training loss = 0.167\n",
      "Epoch 37: validation loss = 0.155\n",
      "Epoch 38: training loss = 0.167\n",
      "Epoch 38: validation loss = 0.154\n",
      "Epoch 39: training loss = 0.165\n",
      "Epoch 39: validation loss = 0.158\n",
      "Epoch 40: training loss = 0.163\n",
      "Epoch 40: validation loss = 0.157\n",
      "Epoch 41: training loss = 0.160\n",
      "Epoch 41: validation loss = 0.156\n",
      "Epoch 42: training loss = 0.157\n",
      "Epoch 42: validation loss = 0.159\n",
      "Epoch 43: training loss = 0.158\n",
      "Epoch 43: validation loss = 0.157\n",
      "Epoch 44: training loss = 0.154\n",
      "Epoch 44: validation loss = 0.158\n",
      "Epoch 45: training loss = 0.152\n",
      "Epoch 45: validation loss = 0.158\n",
      "Epoch 46: training loss = 0.150\n",
      "Epoch 46: validation loss = 0.158\n",
      "Epoch 47: training loss = 0.147\n",
      "Epoch 47: validation loss = 0.161\n",
      "Epoch 48: training loss = 0.148\n",
      "Epoch 48: validation loss = 0.159\n",
      "Epoch 49: training loss = 0.144\n",
      "Epoch 49: validation loss = 0.169\n",
      "Epoch 50: training loss = 0.144\n",
      "Epoch 50: validation loss = 0.160\n",
      "Epoch 51: training loss = 0.142\n",
      "Epoch 51: validation loss = 0.160\n",
      "Epoch 52: training loss = 0.143\n",
      "Epoch 52: validation loss = 0.160\n",
      "Epoch 53: training loss = 0.140\n",
      "Epoch 53: validation loss = 0.161\n",
      "Epoch 54: training loss = 0.139\n",
      "Epoch 54: validation loss = 0.159\n",
      "Epoch 55: training loss = 0.139\n",
      "Epoch 55: validation loss = 0.163\n",
      "Epoch 56: training loss = 0.138\n",
      "Epoch 56: validation loss = 0.166\n",
      "Epoch 57: training loss = 0.133\n",
      "Epoch 57: validation loss = 0.161\n",
      "Epoch 58: training loss = 0.133\n",
      "Epoch 58: validation loss = 0.164\n",
      "Epoch 59: training loss = 0.133\n",
      "Epoch 59: validation loss = 0.167\n",
      "Epoch 60: training loss = 0.131\n",
      "Epoch 60: validation loss = 0.168\n",
      "Epoch 61: training loss = 0.129\n",
      "Epoch 61: validation loss = 0.165\n",
      "Epoch 62: training loss = 0.129\n",
      "Epoch 62: validation loss = 0.171\n",
      "Epoch 63: training loss = 0.129\n",
      "Epoch 63: validation loss = 0.167\n",
      "Epoch 64: training loss = 0.127\n",
      "Epoch 64: validation loss = 0.168\n",
      "Epoch 65: training loss = 0.128\n",
      "Epoch 65: validation loss = 0.168\n",
      "Epoch 66: training loss = 0.126\n",
      "Epoch 66: validation loss = 0.170\n",
      "Epoch 67: training loss = 0.125\n",
      "Epoch 67: validation loss = 0.169\n",
      "Epoch 68: training loss = 0.125\n",
      "Epoch 68: validation loss = 0.171\n",
      "Epoch 69: training loss = 0.125\n",
      "Epoch 69: validation loss = 0.166\n",
      "Epoch 70: training loss = 0.123\n",
      "Epoch 70: validation loss = 0.172\n",
      "Epoch 71: training loss = 0.121\n",
      "Epoch 71: validation loss = 0.171\n",
      "Epoch 72: training loss = 0.121\n",
      "Epoch 72: validation loss = 0.171\n",
      "Epoch 73: training loss = 0.122\n",
      "Epoch 73: validation loss = 0.173\n",
      "Epoch 74: training loss = 0.119\n",
      "Epoch 74: validation loss = 0.170\n",
      "Epoch 75: training loss = 0.120\n",
      "Epoch 75: validation loss = 0.173\n",
      "Epoch 76: training loss = 0.119\n",
      "Epoch 76: validation loss = 0.171\n",
      "Epoch 77: training loss = 0.118\n",
      "Epoch 77: validation loss = 0.176\n",
      "Epoch 78: training loss = 0.118\n",
      "Epoch 78: validation loss = 0.179\n",
      "Epoch 79: training loss = 0.117\n",
      "Epoch 79: validation loss = 0.177\n",
      "Epoch 80: training loss = 0.116\n",
      "Epoch 80: validation loss = 0.185\n",
      "Epoch 81: training loss = 0.116\n",
      "Epoch 81: validation loss = 0.181\n",
      "Epoch 82: training loss = 0.117\n",
      "Epoch 82: validation loss = 0.175\n",
      "Epoch 83: training loss = 0.112\n",
      "Epoch 83: validation loss = 0.181\n",
      "Epoch 84: training loss = 0.113\n",
      "Epoch 84: validation loss = 0.174\n",
      "Epoch 85: training loss = 0.114\n",
      "Epoch 85: validation loss = 0.183\n",
      "Epoch 86: training loss = 0.113\n",
      "Epoch 86: validation loss = 0.177\n",
      "Epoch 87: training loss = 0.112\n",
      "Epoch 87: validation loss = 0.176\n",
      "Epoch 88: training loss = 0.113\n",
      "Epoch 88: validation loss = 0.176\n",
      "Epoch 89: training loss = 0.111\n",
      "Epoch 89: validation loss = 0.176\n",
      "Epoch 90: training loss = 0.110\n",
      "Epoch 90: validation loss = 0.176\n",
      "Epoch 91: training loss = 0.111\n",
      "Epoch 91: validation loss = 0.181\n",
      "Epoch 92: training loss = 0.110\n",
      "Epoch 92: validation loss = 0.178\n",
      "Epoch 93: training loss = 0.110\n",
      "Epoch 93: validation loss = 0.177\n",
      "Epoch 94: training loss = 0.110\n",
      "Epoch 94: validation loss = 0.179\n",
      "Epoch 95: training loss = 0.108\n",
      "Epoch 95: validation loss = 0.181\n",
      "Epoch 96: training loss = 0.107\n",
      "Epoch 96: validation loss = 0.184\n",
      "Epoch 97: training loss = 0.109\n",
      "Epoch 97: validation loss = 0.179\n",
      "Epoch 98: training loss = 0.109\n",
      "Epoch 98: validation loss = 0.180\n",
      "Epoch 99: training loss = 0.108\n",
      "Epoch 99: validation loss = 0.187\n",
      "Epoch 100: training loss = 0.107\n",
      "Epoch 100: validation loss = 0.181\n"
     ]
    }
   ],
   "source": [
    "# 5. Train the model\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for inputs, targets, lengths in train_loader:\n",
    "        inputs, targets, lengths = inputs.to(device), targets.to(device), lengths.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(inputs)     # (seq_len, tag_count)\n",
    "\n",
    "        B, T = inputs.size()\n",
    "        # Create mask for real tokens (non-padding)\n",
    "        mask = create_mask(lengths, T)\n",
    "        \n",
    "        # Apply mask to compute loss only on real tokens\n",
    "        logits_masked = logits[mask]\n",
    "        targets_masked = targets[mask]\n",
    "        \n",
    "        # Calculate loss on masked data\n",
    "        loss = criterion(logits_masked, targets_masked)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * B\n",
    "    avg_loss = total_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}: training loss = {avg_loss:.3f}\")\n",
    "    # Validate on validation set\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets, lengths in test_loader:\n",
    "            inputs, targets, lengths = inputs.to(device), targets.to(device), lengths.to(device)\n",
    "            logits = model(inputs)\n",
    "\n",
    "            B, T = inputs.size()\n",
    "\n",
    "            # Create mask for real tokens (non-padding)\n",
    "            mask = create_mask(lengths, T)\n",
    "            \n",
    "            # Apply mask to compute loss only on real tokens\n",
    "            logits_masked = logits[mask]\n",
    "            targets_masked = targets[mask]\n",
    "            \n",
    "            # Calculate loss on masked data\n",
    "            loss = criterion(logits_masked, targets_masked)\n",
    "            total_loss += loss.item() * B\n",
    "            loss = total_loss / len(val_loader.dataset)\n",
    "        print(f\"Epoch {epoch+1}: validation loss = {loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 91.37%\n",
      "Correct tokens: 22911 / 25074\n"
     ]
    }
   ],
   "source": [
    "# 6. Evaluate on test set\n",
    "model.eval()\n",
    "correct_tokens = 0\n",
    "total_tokens = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, targets, lengths in test_loader:\n",
    "        inputs, targets, lengths = inputs.to(device), targets.to(device), lengths.to(device)\n",
    "        batch_size, max_len = inputs.size()\n",
    "        \n",
    "        # Forward pass (without passing lengths if not needed by model architecture)\n",
    "        logits = model(inputs)  # Shape: [batch_size, max_len, num_tags]\n",
    "        \n",
    "        # Get predicted tags\n",
    "        pred_tags = logits.argmax(dim=-1)  # Shape: [batch_size, max_len]\n",
    "        \n",
    "        # Create mask for real tokens (non-padding)\n",
    "        mask = create_mask(lengths, max_len)\n",
    "        \n",
    "        # Count correct predictions only on real tokens\n",
    "        correct_tokens += ((pred_tags == targets) & mask).sum().item()\n",
    "        total_tokens += mask.sum().item()\n",
    "\n",
    "accuracy = correct_tokens / total_tokens\n",
    "print(f\"Test Accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"Correct tokens: {correct_tokens} / {total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ultralytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
