{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.fc1 = nn.Linear(256 * 3 * 3, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.leaky_relu(self.bn1(self.conv1(x))))  # 64x16x16\n",
    "        x = self.pool(F.leaky_relu(self.bn2(self.conv2(x))))   # 128x8x8\n",
    "        x = self.pool(F.leaky_relu(self.bn3(self.conv3(x))))   # 256x4x4\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, z1, z2, label):\n",
    "        euclidean_distance = F.pairwise_distance(z1, z2, keepdim=False)\n",
    "        loss_contrastive = torch.mean((label) * torch.pow(euclidean_distance, 2) +\n",
    "                                    (1-label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "\n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Siamese Dataset Loader\n",
    "class SiameseDataset(Dataset):\n",
    "    def __init__(self, mnist_dataset):\n",
    "        self.mnist_dataset = mnist_dataset\n",
    "        self.label_to_indices = {}\n",
    "        for idx, (_, label) in enumerate(self.mnist_dataset):\n",
    "            if label not in self.label_to_indices:\n",
    "                self.label_to_indices[label] = []\n",
    "            self.label_to_indices[label].append(idx)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img1, label1 = self.mnist_dataset[index]\n",
    "        same_class = random.randint(0, 1)\n",
    "        if same_class:\n",
    "            idx2 = random.choice(self.label_to_indices[label1])\n",
    "        else:\n",
    "            diff_label = random.choice(list(self.label_to_indices.keys()))\n",
    "            while diff_label == label1:\n",
    "                diff_label = random.choice(list(self.label_to_indices.keys()))\n",
    "            idx2 = random.choice(self.label_to_indices[diff_label])\n",
    "        img2, label2 = self.mnist_dataset[idx2]\n",
    "        label = 1 if label1 == label2 else 0\n",
    "        return img1, img2, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mnist_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "mnist_train = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "mnist_test = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets and dataloaders\n",
    "train_dataset = SiameseDataset(mnist_train)\n",
    "test_dataset = SiameseDataset(mnist_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model, loss and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SiameseNetwork(in_channels=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SiameseNetwork(\n",
       "  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=2304, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc3): Linear(in_features=256, out_features=64, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.constant_(m.bias, 0.01)\n",
    "\n",
    "model.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = ContrastiveLoss(margin=2).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_abs_changes(model, prev_params):\n",
    "    \"\"\"\n",
    "    Compute the sum of absolute changes in model parameters.\n",
    "    :param model: The neural network model.\n",
    "    :param prev_params: Dictionary storing the previous parameter values.\n",
    "    :return: Total sum of absolute changes.\n",
    "    \"\"\"\n",
    "    total_change = 0.0\n",
    "    for name, param in model.named_parameters():\n",
    "        if name in prev_params:\n",
    "            # Compute absolute change\n",
    "            change = torch.sum(torch.abs(param.data - prev_params[name])).item()\n",
    "            total_change += change\n",
    "        # Update previous parameters\n",
    "        prev_params[name] = param.data.clone()\n",
    "    return total_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 0.7766494184098345, Param Change Sum: 91876.6931\n",
      "Epoch 2/50, Loss: 0.23984754662564461, Param Change Sum: 4784.5253\n",
      "Epoch 3/50, Loss: 0.17942292087889733, Param Change Sum: 4358.9033\n",
      "Epoch 4/50, Loss: 0.15445082289107304, Param Change Sum: 4808.3423\n",
      "Epoch 5/50, Loss: 0.13324828756616472, Param Change Sum: 4526.8121\n",
      "Epoch 6/50, Loss: 0.11485166974524234, Param Change Sum: 3922.0429\n",
      "Epoch 7/50, Loss: 0.1016930083962197, Param Change Sum: 4171.3417\n",
      "Epoch 8/50, Loss: 0.09220289749668, Param Change Sum: 4764.7257\n",
      "Epoch 9/50, Loss: 0.08129568149117714, Param Change Sum: 4319.3327\n",
      "Epoch 10/50, Loss: 0.07879835069179535, Param Change Sum: 6189.5499\n",
      "Epoch 11/50, Loss: 0.06329500822627798, Param Change Sum: 4838.7850\n",
      "Epoch 12/50, Loss: 0.05416015106788341, Param Change Sum: 4847.3699\n",
      "Epoch 13/50, Loss: 0.05107551252746836, Param Change Sum: 6154.5487\n",
      "Epoch 14/50, Loss: 0.04576607366350103, Param Change Sum: 6050.1599\n",
      "Epoch 15/50, Loss: 0.03812443917577571, Param Change Sum: 5465.0312\n",
      "Epoch 16/50, Loss: 0.033551012193585965, Param Change Sum: 5759.3198\n",
      "Epoch 17/50, Loss: 0.031719386169409496, Param Change Sum: 7334.4450\n",
      "Epoch 18/50, Loss: 0.02635850840585029, Param Change Sum: 6257.9080\n",
      "Epoch 19/50, Loss: 0.024177676732552812, Param Change Sum: 6746.2440\n",
      "Epoch 20/50, Loss: 0.023467183866082354, Param Change Sum: 7017.1675\n",
      "Epoch 21/50, Loss: 0.020934317681066534, Param Change Sum: 7161.6883\n",
      "Epoch 22/50, Loss: 0.017818967730520253, Param Change Sum: 8189.2136\n",
      "Epoch 23/50, Loss: 0.014759305226517484, Param Change Sum: 7192.4167\n",
      "Epoch 24/50, Loss: 0.012104391235303371, Param Change Sum: 7439.7540\n",
      "Epoch 25/50, Loss: 0.012067033674449045, Param Change Sum: 8194.0633\n",
      "Epoch 26/50, Loss: 0.015728104697104464, Param Change Sum: 10132.3507\n",
      "Epoch 27/50, Loss: 0.011016815754168845, Param Change Sum: 10110.6582\n",
      "Epoch 28/50, Loss: 0.01067811465386222, Param Change Sum: 10068.4178\n",
      "Epoch 29/50, Loss: 0.009485320059305176, Param Change Sum: 9733.5438\n",
      "Epoch 30/50, Loss: 0.00835137677458214, Param Change Sum: 9441.4867\n",
      "Epoch 31/50, Loss: 0.0072786034208702595, Param Change Sum: 9950.7092\n",
      "Epoch 32/50, Loss: 0.00836231313606209, Param Change Sum: 10755.0101\n",
      "Epoch 33/50, Loss: 0.009611048570219823, Param Change Sum: 12081.8263\n",
      "Epoch 34/50, Loss: 0.006577803687430284, Param Change Sum: 10976.4707\n",
      "Epoch 35/50, Loss: 0.0072067762858808995, Param Change Sum: 12693.6276\n",
      "Epoch 36/50, Loss: 0.008090101577777495, Param Change Sum: 13971.8480\n",
      "Epoch 37/50, Loss: 0.0077535098478039525, Param Change Sum: 16310.9726\n",
      "Epoch 38/50, Loss: 0.005555801478491977, Param Change Sum: 12785.9436\n",
      "Epoch 39/50, Loss: 0.007789213916267011, Param Change Sum: 18495.7416\n",
      "Epoch 40/50, Loss: 0.00648076154980214, Param Change Sum: 15709.7009\n",
      "Epoch 41/50, Loss: 0.004692389388033684, Param Change Sum: 16606.6939\n",
      "Epoch 42/50, Loss: 0.004524263129767744, Param Change Sum: 13202.3229\n",
      "Epoch 43/50, Loss: 0.00903758649872814, Param Change Sum: 21171.2807\n",
      "Epoch 44/50, Loss: 0.007568187357738931, Param Change Sum: 19615.0973\n",
      "Epoch 45/50, Loss: 0.004722354303490608, Param Change Sum: 13902.7351\n",
      "Epoch 46/50, Loss: 0.0037283747813644562, Param Change Sum: 15806.7691\n",
      "Epoch 47/50, Loss: 0.0040153646182784055, Param Change Sum: 14735.9387\n",
      "Epoch 48/50, Loss: 0.004176030241874383, Param Change Sum: 15039.6904\n",
      "Epoch 49/50, Loss: 0.003942749707989315, Param Change Sum: 14419.0093\n",
      "Epoch 50/50, Loss: 0.0030141607434686986, Param Change Sum: 13802.7903\n"
     ]
    }
   ],
   "source": [
    "prev_params = {name: param.data.clone() for name, param in model.named_parameters()}\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for img1, img2, label in train_loader:\n",
    "        img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        z1 = model(img1)\n",
    "        z2 = model(img2)\n",
    "        loss = criterion(z1, z2, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        # with torch.no_grad():\n",
    "        #     pos_dist = F.pairwise_distance(z1[label==1], z2[label==1])\n",
    "        #     neg_dist = F.pairwise_distance(z1[label==0], z2[label==0])\n",
    "        #     print(f\"Pos dist: {pos_dist.mean():.2f}, Neg dist: {neg_dist.mean():.2f}\")\n",
    "\n",
    "    # Sum absolute values of parameters\n",
    "    param_change_sum = sum_abs_changes(model, prev_params)\n",
    "\n",
    "    prev_params = {name: param.data.clone() for name, param in model.named_parameters()}\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader)}, Param Change Sum: {param_change_sum:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding the test set\n",
    "model.eval()\n",
    "test_embeddings = []\n",
    "test_images = []\n",
    "test_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for img, label in DataLoader(mnist_test, batch_size=256):\n",
    "        img = img.to(device)\n",
    "        z = model(img)\n",
    "        test_embeddings.append(z.cpu().numpy())\n",
    "        test_images.append(img.cpu().numpy())\n",
    "        test_labels.append(label.numpy())\n",
    "\n",
    "test_embeddings = np.concatenate(test_embeddings)\n",
    "test_images = np.concatenate(test_images)\n",
    "test_labels = np.concatenate(test_labels)\n",
    "\n",
    "\n",
    "train_embeddings = []\n",
    "train_images = []\n",
    "train_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for img, label in DataLoader(mnist_train, batch_size=256):\n",
    "        img = img.to(device)\n",
    "        z = model(img)\n",
    "        train_embeddings.append(z.cpu().numpy())\n",
    "        train_images.append(img.cpu().numpy())\n",
    "        train_labels.append(label.numpy())\n",
    "\n",
    "train_embeddings = np.concatenate(train_embeddings)\n",
    "train_images = np.concatenate(train_images)\n",
    "train_labels = np.concatenate(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN classification accuracy in embedding space: 99.21%\n"
     ]
    }
   ],
   "source": [
    "# KNN Classifier\n",
    "knn_embedding = KNeighborsClassifier(n_neighbors=20)\n",
    "knn_embedding.fit(train_embeddings, train_labels)\n",
    "# accuracy = knn_embedding.score(train_embeddings, train_labels)\n",
    "accuracy = knn_embedding.score(test_embeddings, test_labels)\n",
    "print(f\"KNN classification accuracy in embedding space: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN classification accuracy in image space: 96.25%\n"
     ]
    }
   ],
   "source": [
    "# KNN Classifier\n",
    "knn_images = KNeighborsClassifier(n_neighbors=20)\n",
    "knn_images.fit(train_images.reshape(train_images.shape[0], -1), train_labels)\n",
    "# accuracy = knn_images.score(train_images.reshape(train_images.shape[0], -1), train_labels)\n",
    "accuracy = knn_images.score(test_images.reshape(test_images.shape[0], -1), test_labels)\n",
    "print(f\"KNN classification accuracy in image space: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basicsr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
