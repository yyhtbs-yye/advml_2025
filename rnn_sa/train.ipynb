{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.acl_imdb_dataset import build_dataloader\n",
    "from model import SentimentModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "EPOCHS = 100\n",
    "EARLY_STOP_PATIENCE = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split: 22500 training samples, 2500 validation samples\n"
     ]
    }
   ],
   "source": [
    "train_dataset_path = \"/home/admyyh/python_workspace/advml/rnn_sa/aclImdb/train\"\n",
    "test_dataset_path = \"/home/admyyh/python_workspace/advml/rnn_sa/aclImdb/test\"\n",
    "vocab_path = \"/home/admyyh/python_workspace/advml/rnn_sa/word_vocab_norm_None_stop_False.json\"\n",
    "\n",
    "train_loader, validation_loader = build_dataloader(\n",
    "    root_folder=train_dataset_path, vocab_file=vocab_path,\n",
    "    batch_size=32,  # Using batch size 8 for testing\n",
    "    shuffle=True,\n",
    "    split=True, val_ratio=0.1, random_seed=42,\n",
    ")\n",
    "\n",
    "test_loader = build_dataloader(\n",
    "    root_folder=test_dataset_path, vocab_file=vocab_path,\n",
    "    batch_size=32,  # Using batch size 8 for testing\n",
    "    shuffle=False,\n",
    "    split=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(vocab_path, 'r') as f:\n",
    "    vocab_data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentModel(\n",
       "  (embed): Embedding(99928, 64, padding_idx=0)\n",
       "  (dropout_embed): Dropout(p=0.5, inplace=False)\n",
       "  (lstm): LSTM(64, 64, batch_first=True)\n",
       "  (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout_lstm): Dropout(p=0.5, inplace=False)\n",
       "  (fc): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SentimentModel(vocab_size=len(vocab_data['word_vocab']))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  # tag classification loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  Train Loss: 0.6949 | Train Acc: 53.53%\n",
      "  Val Loss: 0.6626 | Val Acc: 61.64%\n",
      "Epoch 2/100\n",
      "  Train Loss: 0.6283 | Train Acc: 66.35%\n",
      "  Val Loss: 0.5641 | Val Acc: 74.84%\n",
      "Epoch 3/100\n",
      "  Train Loss: 0.5714 | Train Acc: 72.65%\n",
      "  Val Loss: 0.6788 | Val Acc: 67.60%\n",
      "Epoch 4/100\n",
      "  Train Loss: 0.5238 | Train Acc: 76.26%\n",
      "  Val Loss: 0.5407 | Val Acc: 75.60%\n",
      "Epoch 5/100\n",
      "  Train Loss: 0.4472 | Train Acc: 80.99%\n",
      "  Val Loss: 0.6122 | Val Acc: 79.76%\n",
      "Epoch 6/100\n",
      "  Train Loss: 0.3963 | Train Acc: 83.50%\n",
      "  Val Loss: 0.4857 | Val Acc: 82.40%\n",
      "Epoch 7/100\n",
      "  Train Loss: 0.3578 | Train Acc: 85.41%\n",
      "  Val Loss: 0.4175 | Val Acc: 85.36%\n",
      "Epoch 8/100\n",
      "  Train Loss: 0.3220 | Train Acc: 87.24%\n",
      "  Val Loss: 0.4394 | Val Acc: 86.04%\n",
      "Epoch 9/100\n",
      "  Train Loss: 0.2846 | Train Acc: 89.11%\n",
      "  Val Loss: 0.3947 | Val Acc: 88.04%\n",
      "Epoch 10/100\n",
      "  Train Loss: 0.2538 | Train Acc: 90.33%\n",
      "  Val Loss: 0.5191 | Val Acc: 86.92%\n",
      "Epoch 11/100\n",
      "  Train Loss: 0.2291 | Train Acc: 91.51%\n",
      "  Val Loss: 0.4442 | Val Acc: 87.92%\n",
      "Epoch 12/100\n",
      "  Train Loss: 0.2100 | Train Acc: 92.38%\n",
      "  Val Loss: 0.5085 | Val Acc: 87.36%\n",
      "Epoch 13/100\n",
      "  Train Loss: 0.1797 | Train Acc: 93.49%\n",
      "  Val Loss: 0.5057 | Val Acc: 87.36%\n",
      "Epoch 14/100\n",
      "  Train Loss: 0.1675 | Train Acc: 94.13%\n",
      "  Val Loss: 0.5519 | Val Acc: 87.48%\n",
      "Epoch 15/100\n",
      "  Train Loss: 0.1504 | Train Acc: 94.66%\n",
      "  Val Loss: 0.6362 | Val Acc: 87.32%\n",
      "Epoch 16/100\n",
      "  Train Loss: 0.1402 | Train Acc: 95.11%\n",
      "  Val Loss: 0.6149 | Val Acc: 87.00%\n",
      "Epoch 17/100\n",
      "  Train Loss: 0.1262 | Train Acc: 95.67%\n",
      "  Val Loss: 0.6029 | Val Acc: 86.80%\n",
      "Epoch 18/100\n",
      "  Train Loss: 0.1113 | Train Acc: 96.24%\n",
      "  Val Loss: 0.6870 | Val Acc: 87.00%\n",
      "Epoch 19/100\n",
      "  Train Loss: 0.1043 | Train Acc: 96.54%\n",
      "  Val Loss: 0.6661 | Val Acc: 87.12%\n",
      "Epoch 20/100\n",
      "  Train Loss: 0.0982 | Train Acc: 96.73%\n",
      "  Val Loss: 0.6595 | Val Acc: 87.04%\n",
      "Epoch 21/100\n",
      "  Train Loss: 0.0891 | Train Acc: 97.08%\n",
      "  Val Loss: 0.6714 | Val Acc: 87.36%\n",
      "Epoch 22/100\n",
      "  Train Loss: 0.0800 | Train Acc: 97.44%\n",
      "  Val Loss: 0.6823 | Val Acc: 87.36%\n",
      "Epoch 23/100\n",
      "  Train Loss: 0.0778 | Train Acc: 97.40%\n",
      "  Val Loss: 0.6974 | Val Acc: 87.36%\n",
      "Epoch 24/100\n",
      "  Train Loss: 0.0670 | Train Acc: 97.87%\n",
      "  Val Loss: 0.7785 | Val Acc: 87.56%\n",
      "Epoch 25/100\n",
      "  Train Loss: 0.0675 | Train Acc: 97.89%\n",
      "  Val Loss: 0.7306 | Val Acc: 87.36%\n",
      "Epoch 26/100\n",
      "  Train Loss: 0.0602 | Train Acc: 98.13%\n",
      "  Val Loss: 0.7775 | Val Acc: 87.48%\n",
      "Epoch 27/100\n",
      "  Train Loss: 0.0581 | Train Acc: 98.12%\n",
      "  Val Loss: 0.8301 | Val Acc: 86.68%\n",
      "Epoch 28/100\n",
      "  Train Loss: 0.0514 | Train Acc: 98.45%\n",
      "  Val Loss: 0.8155 | Val Acc: 87.24%\n",
      "Epoch 29/100\n",
      "  Train Loss: 0.0504 | Train Acc: 98.46%\n",
      "  Val Loss: 0.8055 | Val Acc: 87.20%\n",
      "Epoch 30/100\n",
      "  Train Loss: 0.0506 | Train Acc: 98.41%\n",
      "  Val Loss: 0.8614 | Val Acc: 86.72%\n",
      "Epoch 31/100\n",
      "  Train Loss: 0.0470 | Train Acc: 98.56%\n",
      "  Val Loss: 0.8627 | Val Acc: 86.88%\n",
      "Epoch 32/100\n",
      "  Train Loss: 0.0411 | Train Acc: 98.76%\n",
      "  Val Loss: 0.8336 | Val Acc: 87.44%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     24\u001b[39m loss.backward()\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Gradient clipping (moved inside the loop)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Update weights\u001b[39;00m\n\u001b[32m     30\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ultralytics/lib/python3.11/site-packages/torch/nn/utils/clip_grad.py:46\u001b[39m, in \u001b[36mclip_grad_norm_\u001b[39m\u001b[34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[39m\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.tensor(\u001b[32m0.\u001b[39m)\n\u001b[32m     44\u001b[39m first_device = grads[\u001b[32m0\u001b[39m].device\n\u001b[32m     45\u001b[39m grouped_grads: Dict[Tuple[torch.device, torch.dtype], List[List[Tensor]]] \\\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     = _group_tensors_by_device_and_dtype([\u001b[43m[\u001b[49m\u001b[43mg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m]\u001b[49m])  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m norm_type == inf:\n\u001b[32m     49\u001b[39m     norms = [torch.linalg.vector_norm(g.detach(), inf).to(first_device) \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m grads]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ultralytics/lib/python3.11/site-packages/torch/nn/utils/clip_grad.py:46\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.tensor(\u001b[32m0.\u001b[39m)\n\u001b[32m     44\u001b[39m first_device = grads[\u001b[32m0\u001b[39m].device\n\u001b[32m     45\u001b[39m grouped_grads: Dict[Tuple[torch.device, torch.dtype], List[List[Tensor]]] \\\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     = _group_tensors_by_device_and_dtype([[\u001b[43mg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m grads]])  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m norm_type == inf:\n\u001b[32m     49\u001b[39m     norms = [torch.linalg.vector_norm(g.detach(), inf).to(first_device) \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m grads]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    for inputs, targets, lengths in train_loader:\n",
    "        inputs, targets, lengths = inputs.to(device), targets.to(device), lengths.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(inputs, lengths)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(logits, targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping (moved inside the loop)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track statistics\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        train_total += targets.size(0)\n",
    "        train_correct += (predicted == targets).sum().item()\n",
    "    \n",
    "    # Calculate average training metrics\n",
    "    avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets, lengths in validation_loader:\n",
    "            inputs, targets, lengths = inputs.to(device), targets.to(device), lengths.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(inputs, lengths)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(logits, targets)\n",
    "            \n",
    "            # Track statistics\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            val_total += targets.size(0)\n",
    "            val_correct += (predicted == targets).sum().item()\n",
    "    \n",
    "    # Calculate average validation metrics\n",
    "    avg_val_loss = val_loss / len(validation_loader.dataset)\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "    \n",
    "    # Print epoch results\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    print(f\"  Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.2f}%\")\n",
    "    print(f\"  Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.2f}%\")\n",
    "    \n",
    "    # # Early stopping check\n",
    "    # if avg_val_loss < best_val_loss:\n",
    "    #     best_val_loss = avg_val_loss\n",
    "    #     patience_counter = 0\n",
    "    #     # Save the best model\n",
    "    #     torch.save(model.state_dict(), 'best_sentiment_model.pt')\n",
    "    #     print(\"  Saved best model!\")\n",
    "    # else:\n",
    "    #     patience_counter += 1\n",
    "    #     print(f\"  No improvement for {patience_counter} epochs\")\n",
    "        \n",
    "    # if patience_counter >= EARLY_STOP_PATIENCE:\n",
    "    #     print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "    #     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ultralytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
